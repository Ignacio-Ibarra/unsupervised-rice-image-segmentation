{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-12 17:26:47.962131: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-10-12 17:26:47.996213: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-10-12 17:26:48.232339: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-10-12 17:26:48.234447: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-12 17:26:49.899816: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import time\n",
    "warnings.filterwarnings(\"ignore\", \"is_categorical_dtype\")\n",
    "warnings.filterwarnings(\"ignore\", \"use_inf_as_na\")\n",
    "\n",
    "# Normalización\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Reducción de Dimensionalidad\n",
    "from umap import UMAP\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Fuzzy Clustering\n",
    "import skfuzzy as fuzz\n",
    "\n",
    "# Metricas\n",
    "from sklearn.metrics import rand_score, adjusted_rand_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seleccionar_muestra(sample_length = None, input_folder=\"output/datasets/\", prefijo=\"features_completos_\", sufijo=\".csv.gz\"):\n",
    "    DEFAULT_LENGTH = 15000 \n",
    "    \n",
    "    clases = [\"Arborio\", \"Basmati\", \"Ipsala\", \"Jasmine\", \"Karacadag\"]\n",
    "       \n",
    "    sample_df = pd.DataFrame()\n",
    "    for clase in clases:\n",
    "        path = input_folder+prefijo+clase+sufijo\n",
    "        ids = np.random.choice(np.arange(0,DEFAULT_LENGTH,1), size=sample_length, replace=False)\n",
    "        df = pd.read_csv(path) \n",
    "        df = df.loc[ids, :].reset_index(drop=True)\n",
    "        sample_df = pd.concat([sample_df, df], axis=0)\n",
    "    print(f\"Stratified Sample of {sample_df.shape[0]}\")\n",
    "    return sample_df \n",
    "\n",
    "def seleccionar_features(X:pd.DataFrame, features:str): \n",
    "\n",
    "    # feautres : str puede ser 'morfologicos', 'conv2d', 'both'\n",
    "    \n",
    "    default_cols = ['image_id','class_name']\n",
    "    morphological_features = ['area','eccentricity','perimeter', 'orientation','axis_major_length','axis_minor_length']\n",
    "    conv2d_features = [str(i) for i in range(4096)]\n",
    "    \n",
    "    if features == \"morfologicos\": \n",
    "        sample_features = X.drop(columns = default_cols + conv2d_features)\n",
    "        \n",
    "    elif features == \"conv2d\": \n",
    "        sample_features = X.drop(columns = default_cols + morphological_features)\n",
    "    \n",
    "    elif features == \"both\":\n",
    "        sample_features = X.drop(columns = default_cols)\n",
    "    else: \n",
    "        raise ValueError(\"El parámetro 'features' solo acepta 'morfologicos', 'conv2d', 'both'\")\n",
    "    sample_labels = X.class_name.to_list()\n",
    "    return sample_features, sample_labels\n",
    "\n",
    "     \n",
    "def hacer_reduccion(X:pd.DataFrame, normalizacion:bool, metodo:str, umap_params:dict):\n",
    "\n",
    "    # method : str puede ser 'pca', 'umap', 'both'\n",
    "    # umap_params: dict {'n_neighbors':int, 'min_dist':float, 'n_components':int, 'metric':str}\n",
    "    \n",
    "    if normalizacion: \n",
    "        scaler = StandardScaler()\n",
    "        X = scaler.fit_transform(X)\n",
    "    \n",
    "    if (metodo == \"pca\") or (metodo == 'both'):\n",
    "        print(\"Reduction Method PCA\") \n",
    "        DEFAULT_MIN_COMP = 100\n",
    "        ncols = X.shape[1]\n",
    "        n_components = np.min([DEFAULT_MIN_COMP, ncols])\n",
    "\n",
    "        pca = PCA(n_components=n_components)\n",
    "        scaled_pca = pca.fit(X)\n",
    "        X = pca.transform(X)\n",
    "\n",
    "    elif (metodo == 'umap') or (metodo == 'both'): \n",
    "        print(\"Reduction Method UMAP\") \n",
    "        if umap_params:\n",
    "            n_neighbors = umap_params['n_neighbors']\n",
    "            min_dist = umap_params['min_dist']\n",
    "            n_components = umap_params['n_components']\n",
    "            metric = umap_params['metric']\n",
    "            umap = UMAP(n_neighbors=n_neighbors,\n",
    "                        min_dist=min_dist,\n",
    "                        n_components=n_components,\n",
    "                        metric=metric)\n",
    "        else:\n",
    "            umap = UMAP()\n",
    "        X = umap.fit_transform(X)\n",
    "    else: \n",
    "        raise ValueError(\"El parámetro 'metodo' solo acepta  'pca', 'umap', 'both'\") \n",
    "\n",
    "    return X       \n",
    "        \n",
    "\n",
    "def obtener_fuzzy_clusters(X:pd.DataFrame, fuzzy_params:dict):\n",
    "    \n",
    "    # fuzzy_params: dict {'n_clusters':int, 'm':int}\n",
    "    DEFAULT_ERROR = 0.001\n",
    "    DEFAULT_MAX_ITER = 1000\n",
    "    if fuzzy_params: \n",
    "        n_clusters = fuzzy_params['n_clusters']\n",
    "        m = fuzzy_params['m']\n",
    "        \n",
    "\n",
    "    _, u, _, _, _, _, _ = fuzz.cluster.cmeans(data = X.T, \n",
    "                                                    c = n_clusters, \n",
    "                                                    m = m, \n",
    "                                                    error=DEFAULT_ERROR, \n",
    "                                                    maxiter=DEFAULT_MAX_ITER, \n",
    "                                                    init=None,\n",
    "                                                    seed = 123)\n",
    "    \n",
    "    # Plot assigned clusters, for each data point in training set\n",
    "    cluster_membership = np.argmax(u, axis=0)\n",
    "    print(\"Fuzzy Clustering already finish!!!\")\n",
    "    return cluster_membership\n",
    "\n",
    "\n",
    "\n",
    "def make_grid(*args):\n",
    "    return list(itertools.product(*args))\n",
    "\n",
    "def vanDongen(ct):\n",
    "    n2=2*(sum(ct.apply(sum,axis=1)))\n",
    "    sumi = sum(ct.apply(np.max,axis=1))\n",
    "    sumj = sum(ct.apply(np.max,axis=0))\n",
    "    maxsumi = np.max(ct.apply(sum,axis=1))\n",
    "    maxsumj = np.max(ct.apply(sum,axis=0))\n",
    "    vd = (n2 - sumi - sumj)/(n2 - maxsumi - maxsumj)\n",
    "    return vd\n",
    "\n",
    "def cross_tab(Labels_orig, Labels_clust):\n",
    "     '''crea matriz de confusión para evaluar etiquetado\n",
    "     labels_orig  = etiquetas originales - reales\n",
    "     labels_test  = etiquetas halladeas por el algoritmo'''\n",
    "     tmp = pd.DataFrame({'Labels_orig': Labels_orig, 'Labels_clust': Labels_clust})\n",
    "     ct = pd.crosstab(tmp['Labels_clust'],tmp['Labels_orig']) # Create crosstab: ct\n",
    "     rand = rand_score(Labels_orig, Labels_clust)\n",
    "     arand= adjusted_rand_score(Labels_orig, Labels_clust)\n",
    "     vandon =vanDongen(ct)\n",
    "     print(f'RAND score={rand:.4f}, Ajusted RAND={arand:.4f}, vanDongen={vandon:.4f} cantidad_de_muestras={len(Labels_orig):,d}')\n",
    "     return ct, rand, arand, vandon\n",
    "\n",
    "\n",
    "def grid_search(grid_params:dict):\n",
    "    \n",
    "    n_samples_list = grid_params['n_samples_list']\n",
    "    feature_selection_list = grid_params['feature_selection_list']\n",
    "    reduction_method_list = grid_params['reduction_method_list']\n",
    "    umap_n_neighbors_list = grid_params['umap_params_grid']['n_neighbors']\n",
    "    umap_min_dist_list = grid_params['umap_params_grid']['min_dist']\n",
    "    umap_n_components_list = grid_params['umap_params_grid']['n_components']\n",
    "    umap_metric_list = grid_params['umap_params_grid']['metric']\n",
    "    fuzzy_n_clusters_list = grid_params['fuzzy_params_grid']['n_clusters']\n",
    "    m_list = grid_params['fuzzy_params_grid']['m']\n",
    "    \n",
    "    grid = make_grid(n_samples_list, feature_selection_list, reduction_method_list, \n",
    "                     umap_n_neighbors_list, umap_min_dist_list, umap_n_components_list, umap_metric_list,\n",
    "                     fuzzy_n_clusters_list, m_list)\n",
    "    entries = []\n",
    "    for n, f, r, unn, umd, unc, um, fnc, m in grid:\n",
    "        print(\"Parámetros: \", n, f, r, unn, umd, unc, fnc, m)\n",
    "        start_time1 = time.time()\n",
    "        X = seleccionar_muestra(sample_length=n)\n",
    "        start_time2 = time.time()\n",
    "        sample_features, sample_labels = seleccionar_features(X, features=f)\n",
    "        umap_params = {'n_neighbors':unn, 'min_dist':umd, 'n_components':unc, 'metric':um}\n",
    "        fuzzy_params = {'n_clusters': fnc, 'm': m}\n",
    "        start_time3 = time.time()\n",
    "        reduced_data = hacer_reduccion(X = sample_features, normalizacion=True, metodo=r, umap_params = umap_params)\n",
    "        cluster_membership = obtener_fuzzy_clusters(X = reduced_data, fuzzy_params= fuzzy_params)\n",
    "        _, rand, arand, vandongen = cross_tab(sample_labels, cluster_membership)\n",
    "        #rand = rand_score(labels_true=sample_labels, labels_pred=cluster_membership)\n",
    "        #arand= adjusted_rand_score(labels_true=sample_labels, labels_pred=cluster_membership)\n",
    "        elapsed_time = (time.time() - start_time1)\n",
    "        time_excluding_data_loading = (time.time() - start_time2)\n",
    "        time_excluding_feature_selection = (time.time() - start_time3)\n",
    "        entry = [n*5, f, r, unn, umd, unc, um, fnc, m, rand, arand, vandongen,  elapsed_time, time_excluding_data_loading, time_excluding_feature_selection]\n",
    "#        print(f\"RAND {rand} y ARAND {arand}\")\n",
    "        print(f\"Elapsed time {elapsed_time}\\n\")\n",
    "        entries.append(entry)\n",
    "        \n",
    "    \n",
    "    output_cols = ['n_samples', 'feature_selection','reduction_method',\n",
    "                   'umap_n_neighbors', 'umap_min_dist','umap_n_components','umap_metric',\n",
    "                   'fuzzy_n_clusters','m', 'rand','arand', 'vandongen',\n",
    "                   'elapsed_time', 'time_excluding_data_loading', 'time_excluding_feature_selection']\n",
    "    return pd.DataFrame(entries, columns=output_cols)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En algunos casos tiene seteado parámetros de prueba, hay que ponerle los que están comentados\n",
    "\n",
    "\n",
    "\n",
    "result_grid_search = grid_search(grid_params=grid_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
