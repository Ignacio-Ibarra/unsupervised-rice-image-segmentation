# -*- coding: utf-8 -*-
"""Copia de notebook_zip.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OCv99xKPv5YE0CITP6WPWwomliJox3qC
"""

# clustering and dimension reduction
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import sklearn as sk
from sklearn import metrics

from scipy.cluster.hierarchy import dendrogram
from sklearn.cluster import AgglomerativeClustering

import umap
import umap.plot

#import gower # calcula matriz de distancias gower
import fastcluster # mejora la performance del cluster jerárquico

# para gridsearch
# https://genieclust.gagolewski.com/
# https://doi.org/10.1016/j.softx.2021.100722
import genieclust

# for everything else
import numpy as np
import matplotlib as mpl
import matplotlib.pyplot as plt
import pandas as pd



# trabajo con zip
#from zipfile import ZipFile
#from io import BytesIO

# from google.colab import drive
# drive.mount('/content/drive')

# %%

def grafico_evaluacion(X_train_test, labels_orig, labels_test , nom_test=''):
        '''grafico en el espacio vectorial del pca las clasificaciones realizadas
        X_train_test = coordenadas de lso puntos en PCA -considera slo las primeras 4-
        labels_orig  = etiquetas originales - reales
        labels_test  = etiquetas halladeas por el algoritmo
        nom_test     = texto para aclara en el grafico el nombre del clasificador utilizado
        '''
        
        cmap = mpl.colors.LinearSegmentedColormap.from_list(
            'Custom cmap', plt.cm.tab20(np.arange(5)), 5)

        
        fig, axs = plt.subplots(2,2, figsize=(8,8))
        axs[0,0].scatter(X_train_test[:,0],X_train_test[:,1],s=1,c=labels_test, alpha=0.5, cmap=cmap)
        axs[0,0].set_xlabel('PC1')
        axs[0,0].set_ylabel('PC2')
        axs[0,1].scatter(X_train_test[:,2],X_train_test[:,3],s=1,c=labels_test, alpha=0.5, cmap=cmap)
        axs[0,1].set_xlabel('PC3')
        axs[0,1].set_ylabel('PC4')
        
        
        axs[1,0].scatter(X_train_test[:,0],X_train_test[:,1],s=1,c=labels_orig, alpha=0.5, cmap=cmap)
        axs[1,0].set_xlabel('PC1')
        axs[1,0].set_ylabel('PC2')
        axs[1,1].scatter(X_train_test[:,2],X_train_test[:,3],s=1,c=labels_orig, alpha=0.5, cmap=cmap)
        axs[1,1].set_xlabel('PC3')
        axs[1,1].set_ylabel('PC4')
        
        
        axs[0,0].set_title(f'{nom_test}')
        axs[0,1].set_title(f'{nom_test}')
        axs[1,0].set_title('real')
        axs[1,1].set_title('real')
        fig.set_constrained_layout('constrained')
        return None
    
def cross_tab(Labels_orig, Labels_clust):
    '''crea matriz de confusión para evaluar etiquetado
    labels_orig  = etiquetas originales - reales
    labels_test  = etiquetas halladeas por el algoritmo'''
    tmp = pd.DataFrame({'Labels_orig': Labels_orig, 'Labels_clust': Labels_clust})
    ct = pd.crosstab(tmp['Labels_clust'],tmp['Labels_orig']) # Create crosstab: ct
    rand = metrics.rand_score(Labels_orig, Labels_clust)
    arand= metrics.adjusted_rand_score(Labels_orig, Labels_clust)
    print(f'RAND score={rand:.4f}, Ajusted RAND={arand:.4f}, cantidad_de_muestras={len(Labels_orig):,d}')
    return ct



# %% Paso 2 - Acá separo de la parte que usa el modelo vgg16 y levanto las imágenes ya procesadas
    
def leo_y_consolido_features(input_path='./input/'):
    df1 = pd.read_csv(input_path+'features_Arborio.csv')
    df2 = pd.read_csv(input_path+'features_Basmati.csv')
    df3 = pd.read_csv(input_path+'features_Ipsala.csv')
    df4 = pd.read_csv(input_path+'features_Jasmine.csv')
    df5 = pd.read_csv(input_path+'features_Karacadag.csv')
    features = np.array(pd.concat([df1,df2,df3,df4,df5], axis =0))
    return features
    
def paso_a_pca(features, guardo=False, archivo_guarda=None):
    # reduce the amount of dimensions in the feature vector
    pca = PCA(n_components=100, random_state=22)
    pca.fit(features)
    x = pca.transform(features)
    if guardo:
        archivo_pca = archivo_guarda
        pd.DataFrame(x).to_csv(archivo_pca, index=False)
    # grafico varianza explicada
    ax = plt.subplot()
    ax.plot(100*pca.explained_variance_ / sum(pca.explained_variance_),'.-')
    ax.set_xlim([0,15])
    ax.set_xlabel('PCs')
    ax.set_ylabel('Var. explicada (%)')
    return x

# %%


ARCHIVO_PCA = 'C:/Users/jfgonzalez/Documents/Documentación_maestría/Ciencia_y_tecnologia/feat_pca.csv'
INPUT_P = 'C:/Users/jfgonzalez/Documents/Documentación_maestría/Ciencia_y_tecnologia/unsupervised-rice-image-segmentation/input/'

# x = paso_a_pca(leo_y_consolido_features(input_path=INPUT_P),
#                guardo=True, archivo_guarda=ARCHIVO_PCA)

x = pd.read_csv(ARCHIVO_PCA).to_numpy()

#%%

# como están ordenadas por clase voy a hacer una lista con las etiquetas ahora antes de mezclarlas
clases_dict = {0:'Arborio', 1:'Basmati', 2:'Ipsala', 3:'Jasmine', 4:'Karacadag'}
lista_clases = ([0]*15000 + [1]*15000 + [2]*15000 + [3]*15000 + [4]*15000)


# %% divido dataset

# separo en train_test y validación con sus respectivas etiquetas y los mezclo
X_train_test, X_val, y_train_test, y_val = sk.model_selection.train_test_split(x, lista_clases, test_size=0.2, random_state=42)

#pd.Series(y_train_test).map(clases_dict).value_counts()
org_lab = pd.Series(y_train_test).map(clases_dict)

# %%
# del x, X_val, y_val, lista_clases

#%% aplico kmeans

#kmeans = KMeans(n_clusters=len(unique_labels))
kmeans = KMeans(n_clusters=5)
kmeans.fit(X_train_test)


#%% ploteo

grafico_evaluacion(X_train_test, y_train_test, kmeans.labels_ , nom_test='kmeans')
cross_tab(org_lab, kmeans.labels_)

#%% calculo distancias gower
'''
# cuelga por falta de ram, lo pruebo local
d_meta = gower.gower_matrix(X_train_test)

#%% grafico cluster jerarquico
start_time = time.perf_counter()

xval = pd.Series(y_train_test)
lut = dict(zip(xval.unique(), "rbgyc"))
row_colors = xval.map(lut).to_numpy()

xval = pd.Series(kmeans.labels_)
lut = dict(zip(xval.unique(), "rbgyc"))
col_colors = xval.map(lut).to_numpy()

mapa_clust = sns.clustermap(d_meta, row_colors=row_colors, col_colors=col_colors)

end_time = time.perf_counter()
total_time = end_time - start_time
print(f'Took {total_time:.4f} seconds')
'''

# %% aplico cluster jerárquico

g = genieclust.Genie(n_clusters=5)
labels_genie = g.fit_predict(X_train_test)

# %% evaluo

grafico_evaluacion(X_train_test, y_train_test, labels_genie , nom_test='genie')
cross_tab(org_lab, labels_genie)

#%%

def umap_y_genie_kmeans(X_train_test, n_neighbors=20, min_dist=0.0, n_components=2, metric='euclidean'):
    um = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, n_components=n_components, metric=metric)
    Xred = um.fit_transform(X_train_test)
    
    genie_umap = genieclust.Genie(n_clusters=5).fit_predict(Xred)
    kmeans_umap = KMeans(n_clusters=5, n_init='auto').fit_predict(Xred)
    
    print()
    print('KMEANS', f'n_neighbors={n_neighbors}, min_dist={min_dist}, n_components={n_components}')
    print(cross_tab(org_lab, kmeans_umap))
    print()
    print('HCLUST', f'n_neighbors={n_neighbors}, min_dist={min_dist}, n_components={n_components}')
    print(cross_tab(org_lab, genie_umap))
    
    rand = metrics.rand_score(org_lab, genie_umap)
    arand= metrics.adjusted_rand_score(org_lab, genie_umap)
    
    
    return rand, arand, n_neighbors, min_dist, n_components, Xred



def grid_search():
    lista=[]
    for i in range(5):
        for j in range(2):
            for k in [2,3,4]:
                rand, arand, n_neighbors, min_dist, n_components, Xred = umap_y_genie_kmeans(X_train_test, n_neighbors=(5*(i+1)), min_dist=(j*0.1), n_components=k, metric='euclidean')
                entry = [rand, arand, n_neighbors, min_dist, n_components]
                lista.append(entry)
    
    return pd.DataFrame(lista, columns=['rand', 'arand', 'n_neighbors', 'min_dist', 'n_components'])

# chequeo = grid_search()

# chequeo.to_csv('grid_search_genie.csv')

#%%
'''
um = umap.UMAP(n_neighbors=25, min_dist=0.1, n_components=2, metric='euclidean')
Xred = um.fit_transform(X_train_test)

genie_umap = genieclust.Genie(n_clusters=5).fit_predict(Xred)
kmeans_umap = KMeans(n_clusters=5, n_init='auto').fit_predict(Xred)

umap.plot.points(um, labels=org_lab)
umap.plot.points(um, labels=kmeans_umap)
umap.plot.points(um, labels=genie_umap)

umap.plot.connectivity(um, show_points=True, labels=org_lab)

# %% evaluo

grafico_evaluacion(X_train_test, y_train_test, kmeans_umap , nom_test='umap + kmeans')


print()
print('KMEANS', 'n_neighbors=15, min_dist=01, n_components=2')
print(cross_tab(org_lab, kmeans_umap))
print()
print('HCLUST', 'n_neighbors=15, min_dist=01, n_components=2')
print(cross_tab(org_lab, genie_umap))
'''
# %% después del grid search me quedo con estos parametros.  
    
rand, arand, n_neighbors, min_dist, n_components, Xred =  umap_y_genie_kmeans(X_train_test, n_neighbors=20, min_dist=0.0, n_components=2, metric='euclidean')



#%%

def plot_dendrogram(model, **kwargs):
    # Create linkage matrix and then plot the dendrogram

    # create the counts of samples under each node
    counts = np.zeros(model.children_.shape[0])
    n_samples = len(model.labels_)
    for i, merge in enumerate(model.children_):
        current_count = 0
        for child_idx in merge:
            if child_idx < n_samples:
                current_count += 1  # leaf node
            else:
                current_count += counts[child_idx - n_samples]
        counts[i] = current_count

    linkage_matrix = np.column_stack(
        [model.children_, model.distances_, counts]
    ).astype(float)

    # Plot the corresponding dendrogram
    
    dendrogram(linkage_matrix, **kwargs)
    return linkage_matrix

#%%
def dendro(hclust, color_threshold=150  ):
    fig = plt.figure(figsize=(15,4))
    plt.title("Hierarchical Clustering Dendrogram")
    # plot the top three levels of the dendrogram
    linkage_matrix = plot_dendrogram(hclust, p=3, leaf_rotation=45, truncate_mode="level", color_threshold=color_threshold)
    plt.xlabel("Number of points in node (or index of point if no parenthesis).")
    plt.show()
    return linkage_matrix

# %% ward

hclust_pca = git push(n_clusters=None, linkage='ward', compute_distances=True, distance_threshold=2500 )
hclust_pca.fit(X_train_test)
label_pca = hclust_pca.fit_predict(X_train_test)

print('AgglomerativeClustering', "n_clusters=None, linkage='ward', compute_distances=True, distance_threshold=2500")
print(cross_tab(y_train_test, hclust_pca.labels_))
mx_pca = dendro(hclust_pca, color_threshold=2500)

#%%


hclust_umap = AgglomerativeClustering( n_clusters=None, linkage='ward', compute_distances=True, distance_threshold=350 )

label_umap = hclust_umap.fit_predict(Xred)

print('AgglomerativeClustering UMAP') 
print('AgglomerativeClustering params:',"n_clusters=None, linkage='ward', compute_distances=True, distance_threshold=350")
print('UMAP params:', "n_neighbors=20, min_dist=0.0, n_components=2, metric='euclidean")
print(cross_tab(y_train_test, hclust_umap.labels_))

grafico_evaluacion(X_train_test , y_train_test, hclust_umap.labels_ , nom_test='Agglomerative+UMAP')
mx_umap = dendro(hclust_umap, color_threshold=350)

#%%

a = dendrogram(mx_pca, p=3, leaf_rotation=45, truncate_mode="level", color_threshold=2500)

b= pd.DataFrame(zip([int(x.strip('()')) for x in a['ivl']], a['leaves_color_list']))
b.pivot_table(values=0, index=1, aggfunc='sum')



a = dendrogram(mx_umap, p=3, leaf_rotation=45, truncate_mode="level", color_threshold=350)

b= pd.DataFrame(zip([int(x.strip('()')) for x in a['ivl']], a['leaves_color_list']))
b.pivot_table(values=0, index=1, aggfunc='sum')




#%%

